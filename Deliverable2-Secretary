import math
import random
import matplotlib.pyplot as plt
import numpy as np


# --------------------------------
# Candidate Generator
# --------------------------------

def generate_candidates(n, distribution="uniform"):
    """
    Generate n candidate scores based on the specified distribution.
    - "uniform": Uniformly distributed in [0, 1]
    - "normal": Gaussian with mean 0.5 and std 0.15 (clipped to [0, 1])
    - "exponential": Exponentially distributed (normalized to [0, 1])
    """
    import random

    if distribution == "uniform":
        return [random.random() for _ in range(n)]
    elif distribution == "normal":
        return [min(max(random.gauss(0.5, 0.15), 0), 1) for _ in range(n)]
    elif distribution == "exponential":
        raw = [random.expovariate(1) for _ in range(n)]
        max_val = max(raw)
        return [x / max_val for x in raw] if max_val > 0 else raw
    else:
        raise ValueError("Unsupported distribution type.")



# --------------------------------
# Single Trial Simulation with Estimator Evaluation
# --------------------------------
def secretary_trial(n, k, distribution="uniform"):
    """
    Run one trial of the secretary simulation with a given rejection threshold k.

    Parameters:
      - n: Total number of candidates.
      - k: Number of candidates to reject in the observation phase.
      - distribution: Distribution to generate candidate scores.

    Process:
      1. Generate candidate scores.
      2. Record the true best candidate and its score.
      3. In the observation phase (first k candidates), compute observed_max.
      4. In the selection phase (from k to n-1), select the first candidate with score > observed_max.
         If none qualify, select the last candidate.

    Also, the estimator here is the observed_max (the best seen in the rejection phase).

    Returns:
      - success (bool): True if the selected candidate is the best overall.
      - estimator_error (float): Difference (true_best - observed_max).
      - selected_value (float): The score of the selected candidate.
      - true_best (float): The maximum candidate score.
    """
    candidates = generate_candidates(n, distribution)
    true_best = max(candidates)
    best_index = candidates.index(true_best)

    # Observation phase: reject first k candidates
    if k > 0:
        observed_max = max(candidates[:k])
    else:
        # With k = 0, no observation; set observed_max to a very low value
        observed_max = -float('inf')

    # Selection phase: select the first candidate exceeding observed_max
    selected_index = None
    for i in range(k, n):
        if candidates[i] > observed_max:
            selected_index = i
            break
    if selected_index is None:
        selected_index = n - 1  # if no candidate qualifies, select the last one

    selected_value = candidates[selected_index]
    success = (selected_index == best_index)
    estimator_error = true_best - observed_max  # basic estimator error

    return success, estimator_error, selected_value, true_best


# --------------------------------
# Run Experiment Over a Range of Thresholds
# --------------------------------
def run_threshold_experiment(n, trials, distribution="uniform", threshold_percentages=None):
    """
    Runs the secretary simulation for a range of rejection thresholds.

    Parameters:
      - n: Number of candidates.
      - trials: Number of trials per threshold.
      - distribution: Distribution to generate candidate scores.
      - threshold_percentages: List of rejection thresholds (as fraction of n)
          e.g., [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

    Returns:
      - thresholds (list): Thresholds in percentage (0 to 100).
      - success_rates (list): Average success rate for each threshold.
      - avg_estimation_errors (list): Average estimation error for each threshold.
    """
    if threshold_percentages is None:
        threshold_percentages = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

    thresholds = []
    success_rates = []
    avg_estimation_errors = []

    for thresh_frac in threshold_percentages:
        k = int(n * thresh_frac)
        successes = []
        errors = []
        for _ in range(trials):
            success, est_error, _, _ = secretary_trial(n, k, distribution)
            successes.append(success)
            errors.append(est_error)
        thresholds.append(thresh_frac * 100)
        success_rates.append(np.mean(successes))
        avg_estimation_errors.append(np.mean(errors))
        print(
            f"Threshold: {thresh_frac * 100:.1f}% (k = {k}), Success Rate: {np.mean(successes):.4f}, Estimation Error: {np.mean(errors):.4f}")

    return thresholds, success_rates, avg_estimation_errors


# --------------------------------
# Plotting Results
# --------------------------------
def plot_threshold_results(thresholds, success_rates, avg_estimation_errors):
    plt.figure(figsize=(14, 5))

    # Plot success rate vs. threshold
    plt.subplot(1, 2, 1)
    plt.plot(thresholds, success_rates, marker='o', linestyle='-')
    plt.xlabel("Rejection Threshold (% of n)")
    plt.ylabel("Success Rate")
    plt.title("Success Rate vs. Rejection Threshold")
    plt.grid(True)

    # Plot estimation error vs. threshold
    plt.subplot(1, 2, 2)
    plt.plot(thresholds, avg_estimation_errors, marker='o', linestyle='-', color='red')
    plt.xlabel("Rejection Threshold (% of n)")
    plt.ylabel("Average Estimation Error")
    plt.title("Estimator Error vs. Rejection Threshold")
    plt.grid(True)

    plt.tight_layout()
    plt.show()


# --------------------------------
# Main Execution
# --------------------------------
if __name__ == "__main__":
    n = 1000  # Number of candidates
    trials = 500  # Number of trials per threshold
    distribution = "uniform"  # Change this if you want to test other distributions
    # Define rejection thresholds as percentages (fraction of n)
    threshold_percentages = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

    # Run experiment for a range of thresholds
    thresholds, success_rates, avg_estimation_errors = run_threshold_experiment(n, trials, distribution,
                                                                                threshold_percentages)

    # Plot the results
    plot_threshold_results(thresholds, success_rates, avg_estimation_errors)

    # Find the threshold that gives the maximum success rate
    best_index = np.argmax(success_rates)
    best_threshold = thresholds[best_index]
    best_success_rate = success_rates[best_index]
    print(f"\nMaximum success rate is {best_success_rate:.4f} at a rejection threshold of {best_threshold:.2f}% of n.")


# ------------------------------------------------
# NEW: Compare success rate and estimation error across distributions
# ------------------------------------------------
def compare_distributions(n=100, trials=1000, distributions=["uniform", "normal", "exponential"], threshold_percentages=None):
    import matplotlib.pyplot as plt
    import numpy as np

    if threshold_percentages is None:
        threshold_percentages = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]

    all_results = {}

    for dist in distributions:
        thresholds, success_rates, avg_estimation_errors = run_threshold_experiment(
            n=n,
            trials=trials,
            distribution=dist,
            threshold_percentages=threshold_percentages
        )
        all_results[dist] = {
            "thresholds": thresholds,
            "success_rates": success_rates,
            "errors": avg_estimation_errors
        }

    # Plot Success Rate
    plt.figure(figsize=(10, 5))
    for dist in distributions:
        plt.plot(all_results[dist]["thresholds"], all_results[dist]["success_rates"], marker='o', label=dist)
    plt.xlabel("Rejection Threshold (% of n)")
    plt.ylabel("Success Rate")
    plt.title("Success Rate vs. Rejection Threshold for Different Distributions")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

    # Plot Estimation Error
    plt.figure(figsize=(10, 5))
    for dist in distributions:
        plt.plot(all_results[dist]["thresholds"], all_results[dist]["errors"], marker='o', label=dist)
    plt.xlabel("Rejection Threshold (% of n)")
    plt.ylabel("Estimation Error")
    plt.title("Estimation Error vs. Rejection Threshold for Different Distributions")
    plt.grid(True)
    plt.legend()
    plt.tight_layout()
    plt.show()

if __name__ == "__main__":
    compare_distributions(
        n=100,  # you can try larger like 300 or 1000
        trials=1000,
        distributions=["uniform", "normal", "exponential"],
        threshold_percentages=[0.0, 0.1, 0.2, 0.3, 0.4, 0.5]
    )

